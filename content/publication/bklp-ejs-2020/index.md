---
# Documentation: https://wowchemy.com/docs/managing-content/

title: Adaptive density estimation on bounded domains under mixing conditions
subtitle: ''
summary: ''
authors:
- Karine Bertin
- admin
- Jose R. Léon
- Clémentine Prieur
tags:
- 62g07
- 62h12
- 60g10
- 62p10
- 92d50
categories: []
date: '2020-01-01'
lastmod: 2022-03-31T11:33:37+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-03-31T09:33:36.847478Z'
publication_types:
- '2'
abstract: In this article, we propose a new adaptive estimator for compact supported
  density functions, in the framework of multivariate mixing processes. Several procedures
  have been proposed in the literature to tackle the boundary bias issue encountered
  using classical kernel estimators on the unit d-dimensional hypercube. We extend
  such results to more general bounded domains in R d. We introduce a specific family
  of kernel-type estimators adapted to the estimation of compact supported density
  functions. We then propose a data-driven Goldenshluger and Lepski type procedure
  to jointly select a kernel and a bandwidth. We prove the optimality of our procedure
  in the adaptive framework, stating an oracle-type inequality. We illustrate the
  good behavior of our new class of estimators on simulated data. Finally, we apply
  our procedure to a real dataset.
publication: '*Electronic Journal of Statistics*'
doi: 10.1214/20-EJS1682
---
